{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Differences Between Base Model, Instruction Model, and Chat Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "    <img src=\"https://www.theinsaneapp.com/wp-content/uploads/2023/05/ChatGPT-vs-InstructGPT.jpg\" width=\"1000\"/>\n",
    "</p>\n",
    "Image Source: https://www.theinsaneapp.com/2023/05/instructgpt-vs-chatgpt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Model like davinci-002 primarily generates text based on input without maintaining conversational context. Conversely, the Instruct Model such as gpt-3.5-turbo-instruct offers detailed responses by closely following instructions. Meanwhile, the Chat Model e.g. gpt-3.5-turbo, is designed for natural, interactive conversations, retaining context over multiple turns. Each model, with its unique training data and interaction style, serves distinctive use cases ranging from text generation to interactive customer service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table summarizes some of the \n",
    "comparisons between Base Model, Chat Model and Instruction Model:\n",
    "\n",
    "| Criteria                     | Base Model | Instruct Model | Chat Model  |\n",
    "|------------------------------|---------------------|---------------------|-------------------|\n",
    "| Primary Objective            | Generate text based on input. e.g. GPT base model - davinci-002 | Follow instructions to provide detailed responses. e.g. gpt-3.5-turbo-instruct | Engage in a natural, interactive conversation. e.g. gpt-3.5-turbo |\n",
    "| Interaction Style            | Single-turn; does not maintain context across interactions. | Single-turn; focuses on understanding and executing instructions. | Multi-turn; maintains context over a conversation. |\n",
    "| Training Data                | Diverse text from books, websites, etc. | Instruction-based data; e.g., how-to guides. | Conversational data; dialogues. |\n",
    "| Context Utilization          | Limited or no context from previous interactions. | Primarily focuses on the instruction in the current input. | Utilizes context from previous turns in the conversation. |\n",
    "| Output                       | Broad and general text generation. | Detailed and instructional responses. | More engaging and conversational responses. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how each of these model responds to the use case examples in the follow sections. We will make use of OpenAI API to access these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ending with 'and they all lived happily ever after.\"\n",
      "\n",
      "By Linda Marshall, author and visit www.lindamarshall.net to check out many more free children's writing activities and poetry.\n",
      "\n",
      "Send your writing books to Child-led learning For Drop Everything And Write (DEAW), when most classes are asking students to stop everything and work on writing yearly writing assessment tasks, Alexandra Gardens Primary School in Melbourne is getting excited about an embodied unit of work. Joining in the global project, DEAW: An integrated approach to the learning of reading, writing and speaking, Alexandra Gardens introduces weekly physical activity workshops, blending literacy and movement.\n",
      "\n",
      "Created by staff of the multidisciplinary arts organisation, Roar Contemporary, beginning a March residency at Alexandra Gardens, the project is open to any year one and two class in any school. Set up as a mini-writing festival, DEAW is designed to be a fun way of realising the joy of imaginative storytelling, where 'each morning children arrive at the studio, each with a pencil, each bursting with imagination, ideas and a sense of fun,' says the Alexandra Gardens team. 'We soon see the ties that bind, the girls grab into their imagination world while the boys instinctively move deeper into a world of adventure!'\n",
      "\n",
      "Noting that this 'creates an emotional charge we work with', the Alexandra Gardens school community views this opportunity as 'excellent for making the connections in the brain from our emotional world to our physical world, ' new pathways almost immediately emerge into what might be a story', believes classroom teacher, Kate Smith, who sees an improvement across every genre in the students' work. Students organise and plan writing ahead of time within vocabulary walls, and come with detailed dialogue and sketches of characters, taking control of their learning, using resources provided by staff. 'This is an exciting and inspiring time for the children,' says Kate. 'They'll be developing their writing and communication skills with Roar contemporary – workshops are engaging, dynamic, and playful, and all surrounded by quality art in which the children have genuine freedom of expressive choice.'\n",
      "\n",
      "Contact Roar Contemporary to join the group e: Lora1@mac.com www.roarcontemporary.org.au \"Literacy Starts in the Hands\" Funded by the Balagula Theatre, Billy Bangura, teacher of the year, is seeking an Australian partner to partner him in a writing initiative, literacy starts with the hands. The program works in six schools in Sierra leone and six schools in Liberia.”\n",
      "\n",
      "Contact\n",
      "CPU times: user 31 ms, sys: 14.6 ms, total: 45.6 ms\n",
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"davinci-002\",\n",
    "    prompt=\"Compose a story beginning with 'Once upon a time,'\",\n",
    "    max_tokens=500\n",
    ")\n",
    "generated_text = response['choices'][0]['text']\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Once upon a time, in a far-off kingdom, there lived a brave and adventurous young man named John. He had grown up in the shadow of a great castle, surrounded by thick forests that seemed to stretch on for miles. John dreamed of one day venturing beyond the boundaries of his small village and exploring the world outside.\n",
      "\n",
      "But, unfortunately, John's family was poor and couldn't afford to send him to school. So, he spent most of his days working in the fields, helping his parents grow crops and tend to their livestock. Despite the backbreaking labor, John never lost his spirit for adventure and often found himself daydreaming about the unknown wonders that lay beyond his village.\n",
      "\n",
      "One day, while working in the fields, John noticed a strange figure approaching from the direction of the castle. As it got closer, he could see that it was a knight in shining armor, riding on a majestic white stallion. The knight introduced himself as Sir William and explained that he was on a quest to slay a fierce dragon that had been terrorizing the kingdom.\n",
      "\n",
      "John was awestruck by the knight's bravery and determination. He couldn't resist the urge to ask if he could accompany Sir William on his quest. To his surprise, the knight agreed, knowing that he needed all the help he could get to defeat the dragon.\n",
      "\n",
      "Together, John and Sir William set off towards the castle, eager to face whatever challenges lay ahead. As they journeyed through the dense forests, they encountered all sorts of mythical creatures, from unicorns to fairies and even a giant troll. John was astonished by the many magical wonders he had only read about in books.\n",
      "\n",
      "Eventually, they reached the castle and were greeted by the king himself. He had heard of their quest and offered them his best warriors to join in the fight against the dragon. With a newfound sense of courage and strength, John and the knights marched towards the beast's lair.\n",
      "\n",
      "The dragon, as expected, was fierce and powerful, breathing fire and causing chaos everywhere. But with the help of Sir William and the other brave knights, John was able to contribute to the fight in his own way. He distracted the dragon by using his quick wit and agility, giving Sir William the opportunity to strike the final blow.\n",
      "\n",
      "The kingdom rejoiced as the dragon was defeated, and John was celebrated as a hero. The king even offered him a place in his court, but John politely declined, knowing that there was still so much of\n",
      "CPU times: user 6.09 ms, sys: 4.84 ms, total: 10.9 ms\n",
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Compose a story beginning with 'Once upon a time,'\",\n",
    "    max_tokens=500\n",
    ")\n",
    "story = response['choices'][0]['text']\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small seaside village, there lived a young girl named Lily. Lily was known for her adventurous spirit and love for the ocean. Every day, as she walked along the shore, she would collect seashells, chase seagulls, and dream of the unknown wonders that lay beyond the horizon.\n",
      "\n",
      "One particularly bright morning, while skipping stones across the water, Lily noticed a glimmering object wedged between the rocks. Curiosity sparked in her eyes, she waded into the crystal-clear sea and carefully retrieved the object. It was an ancient-looking scroll, sealed with a wax emblem of a seahorse.\n",
      "\n",
      "Intrigued, Lily rushed home with the scroll and shared her discovery with her grandmother, Beatrice. Wise and knowledgeable about tales of old, Beatrice recognized the emblem as the mark of the Mystic Tides, a legendary group of sea creatures believed to have powers to protect the ocean's harmony.\n",
      "\n",
      "With enthusiasm and a sense of wonder, Lily and Beatrice embarked on a quest to decipher the contents of the scroll. They discovered it was a treasure map, pointing to a fabled island said to be inhabited by the Mystic Tides themselves. The island seemed to hold the key to harmonizing the delicate balance disrupted by human actions on the oceans.\n",
      "\n",
      "Determined to restore the ocean's balance, Lily and Beatrice set sail across the vast blue sea. As they approached the mystical island, they encountered playful dolphins leaping alongside their boat, leading the way. The dolphins seemed to sense their purpose and guided the duo safely through treacherous waves and hidden reefs.\n",
      "\n",
      "Once ashore, Lily and Beatrice were greeted by an enchanting underwater city, shimmering with colorful corals and teeming with diverse marine life. It was the home of the Mystic Tides—an assembly of wise turtles, mystical seahorses, and majestic whales.\n",
      "\n",
      "In the magnificent court of the Mystic Tides, Lily and Beatrice pleaded for their assistance in mending the imbalance caused by pollution, overfishing, and habitat destruction. The sea creatures listened intently and acknowledged the young girl's empathy for their kind.\n",
      "\n",
      "With utmost wisdom, the elder turtles imparted knowledge on sustainable fishing practices, the importance of conserving coral reefs, and the value of spreading awareness among humans. The seahorses, meanwhile, revealed powerful abilities to purify water, making it their mission to cleanse the contaminated seas.\n",
      "\n",
      "Grateful for Lily and Beatrice's visit, the Mystic\n",
      "CPU times: user 5.59 ms, sys: 6.4 ms, total: 12 ms\n",
      "Wall time: 7.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a story that starts with 'Once upon a time,'\"}],\n",
    "    max_tokens=500\n",
    ")\n",
    "story = response['choices'][0]['message']['content']\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello, your family company commission check is ready for pick up. For your safety, and the\n",
      "CPU times: user 35.6 ms, sys: 12.2 ms, total: 47.8 ms\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"davinci-002\",\n",
    "    prompt=\"Translate the following English text to Bengali: 'Hello, how are you?'\",\n",
    "    max_tokens=20\n",
    ")\n",
    "translation = response['choices'][0]['text']\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "হ্যালো, আপনি কে\n",
      "CPU times: user 4.84 ms, sys: 3.5 ms, total: 8.34 ms\n",
      "Wall time: 594 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Translate the following English text to Bengali: 'Hello, how are you?'\",\n",
    "    max_tokens=20\n",
    ")\n",
    "translation = response['choices'][0]['text']\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "হ্যালো, কেমন আছে\n",
      "CPU times: user 4.17 ms, sys: 2.57 ms, total: 6.73 ms\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Translate the following English text to Bengali: 'Hello, how are you?'\"}],\n",
    "    max_tokens=20\n",
    ")\n",
    "translation = response['choices'][0]['message']['content']\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_be_summarized = f\"\"\"\n",
    "Chocolate or cocoa is a food made from roasted and ground cacao seed kernels that is available as a liquid, solid, or paste, either on its own or as a flavoring agent in other foods. Cacao has been consumed in some form since at least the Olmec civilization and the majority of Mesoamerican people, including the Maya and Aztecs, made chocolate beverages.\n",
    "\n",
    "The seeds of the cacao tree have an intense bitter taste and must be fermented to develop the flavor. After fermentation, the seeds are dried, cleaned, and roasted. The shell is removed to produce cocoa nibs, which are then ground to cocoa mass, unadulterated chocolate in rough form. Once the cocoa mass is liquefied by heating, it is called chocolate liquor. The liquor may also be cooled and processed into its two components: cocoa solids and cocoa butter. Baking chocolate, also called bitter chocolate, contains cocoa solids and cocoa butter in varying proportions without any added sugar. Powdered baking cocoa, which contains more fiber than cocoa butter, can be processed with alkali to produce Dutch cocoa. Much of the chocolate consumed today is in the form of sweet chocolate, a combination of cocoa solids, cocoa butter, or added vegetable oils and sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. White chocolate contains cocoa butter, sugar, and milk, but no cocoa solids.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chocolate or cocoa is a food made from roasted and ground cacao seed kernels that is available as a liquid, solid, or paste, either on its own or as a flavoring agent in other foods. Cacao has been consumed in some form since at least the Olmec civilization and the majority of Mesoamerican people, including the Maya and Aztecs, made chocolate beverages.\n",
      "\n",
      "The seeds of the cacao tree have an intense bitter taste and must be fermented to develop the flavor. After fermentation, the seeds are dried, cleaned, and roasted. The shell is removed to produce cocoa nibs, which are then ground to cocoa mass, unadulterated chocolate in rough form. Once the cocoa mass is liquefied by heating, it is called chocolate liquor. The liquor may also be cooled and processed into its two components: cocoa solids and cocoa butter. Baking chocolate, also called bitter chocolate, contains cocoa solids and cocoa butter in varying proportions without any added sugar. Powdered baking cocoa, which contains more fiber than cocoa butter, can be processed with alkali to produce Dutch cocoa. Much of the chocolate consumed today is in the form of sweet chocolate, a combination of cocoa solids, cocoa butter, or added vegetable oils and sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. White chocolate contains cocoa butter, sugar, and milk, but no cocoa solids.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_to_be_summarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following....\n",
      "\n",
      "Give a succinct meaning for the following terminology in your own words:\n",
      "Journal, periodic, and current fabrication. \n",
      "\n",
      "Describe an entrepreneurship that would go under the following categories:\n",
      "Current fabrication.\n",
      "Periodic fabrication.\n",
      "Journal fabrication.\n",
      "\n",
      "Design an excellent\n",
      "CPU times: user 5.88 ms, sys: 6.98 ms, total: 12.9 ms\n",
      "Wall time: 778 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"davinci-002\",\n",
    "    prompt=f\"Summarize the following text: {text_to_be_summarized}\",\n",
    "    max_tokens=50\n",
    ")\n",
    "summary = response['choices'][0]['text']\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chocolate or cocoa is a food made from cacao seeds that can be enjoyed in various forms such as liquid, solid, or paste. It has a long history dating back to the Olmec civilization and was a popular beverage among the Maya and\n",
      "CPU times: user 4.59 ms, sys: 3.25 ms, total: 7.84 ms\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=f\"Summarize the following text: {text_to_be_summarized}\",\n",
    "    max_tokens=50\n",
    ")\n",
    "summary = response['choices'][0]['text']\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chocolate is a food made from roasted and ground cacao seeds. It can be in liquid, solid, or paste form and is used as a flavoring agent in other foods. The cacao seeds have a bitter taste and need to be fermented,\n",
      "CPU times: user 5.29 ms, sys: 3.1 ms, total: 8.39 ms\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Summarize the following text: {text_to_be_summarized}\"}],\n",
    "    max_tokens=50\n",
    ")\n",
    "summary = response['choices'][0]['message']['content']\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The function should take a signed integer…\n",
      "\n",
      "Eigen: The matrix of a relation symbol¶\n",
      "\n",
      "Eigen vectors of a matrix \n",
      "\n",
      "We have seen that MPD’s prime formula language allows definitions in which a predicate indicates a collection of objects. Such a collection is called a relation. A relation is often expressed by a collection of ordered pairs in which the object placed on the left is related to the object placed on the right by some property. The corresponding matrix of the relation represents the positions held by such pairs. Example:\n",
      "\n",
      "pairList A B X Y . . . = {(A, B), (A, X), (B, Y), ... }\n",
      "\n",
      "The positions of the pairs A-B, A-X, ..., can be represented by a three10 rows by three columns matrix10 with elements zero or one0 representing absence or presence of the pair, respectively. Similarly the matrix {B, A, Y} could be represented by a matrix of dimension five by three or thirty rows and columns. The major difference would be the number zero or one and the eleven.\n",
      "\n",
      "Matrix and vector representations of relations can be defined in the flex3 part of a prime formula. Rate the relationship by the matrix multiplication of the concatenation of the two vectors using the dot3 product of vectors, having the dimensions of the arguments, . Raise the dimension of the argument matrices. Example:\n",
      "\n",
      "rel.Obj multiplyVec. U rel.X rel.Y = {(x, i) for x in a . x, for i in j} /\\ {(x, i) for x in b . x, for i in j}\n",
      "\n",
      "The new vector U represents the relations between the objects. The dimension, size, or length of the vectors should be as large as is needed for finding a relation. The reduction of the relation vectorized is not sufficient. By having a large vector the possibility of having a positively related pair of objects will be more or less certain.\n",
      "\n",
      "The following example will make this clearer. We have two sets of objects: humans living in the middle ages and their long misery. They are either given (G) relief (R) by their lords (L), meaning for instance better clothing and food and a reduction of the inhabitant tax, or they are “absolutely” not (N) having a life with loads of miseries.\n",
      "\n",
      "pairlist Human Middle Ages G R N L = {c, d, m, n, p} w/were G(R_N) G(R_L) G(R_G) R\n",
      "CPU times: user 4.18 ms, sys: 2.09 ms, total: 6.28 ms\n",
      "Wall time: 3.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"davinci-002\",\n",
    "    prompt=\"Write a Python function to calculate the factorial of a number.\",\n",
    "    max_tokens=500\n",
    ")\n",
    "code = response['choices'][0]['text']\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def factorial(n):\n",
      "    \n",
      "    # initialize the result variable\n",
      "    result = 1 \n",
      "    \n",
      "    # loop through numbers from 1 to n\n",
      "    for i in range(1, n+1):\n",
      "        \n",
      "        # multiply the result by each number in the range\n",
      "        result *= i \n",
      "        \n",
      "    # return the final result\n",
      "    return result\n",
      "\n",
      "\n",
      "# example\n",
      "print(factorial(5)) # Output: 120\n",
      "CPU times: user 4.21 ms, sys: 2.18 ms, total: 6.38 ms\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Write a Python function to calculate the factorial of a number.\",\n",
    "    max_tokens=500\n",
    ")\n",
    "code = response['choices'][0]['text']\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python function to calculate the factorial of a number using recursion:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "```\n",
      "\n",
      "Here is an example usage of the function:\n",
      "\n",
      "```python\n",
      "num = 5\n",
      "print(f\"The factorial of {num} is: {factorial(num)}\")\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "The factorial of 5 is: 120\n",
      "```\n",
      "\n",
      "Alternatively, you can use a loop to calculate the factorial:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    result = 1\n",
      "    for i in range(1, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "```\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      "num = 5\n",
      "print(f\"The factorial of {num} is: {factorial(num)}\")\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "The factorial of 5 is: 120\n",
      "```\n",
      "CPU times: user 4.27 ms, sys: 3.1 ms, total: 7.37 ms\n",
      "Wall time: 4.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a Python function to calculate the factorial of a number.\"}],\n",
    "    max_tokens=500\n",
    ")\n",
    "code = response['choices'][0]['message']['content']\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Answering Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi, you there. Not speaking? (wind\n",
      "CPU times: user 3.94 ms, sys: 3.34 ms, total: 7.28 ms\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"davinci-002\",\n",
    "    prompt=\"What is the capital of Bangladesh?\",\n",
    "    max_tokens=10\n",
    ")\n",
    "answer = response['choices'][0]['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of Bangladesh is Dhaka.\n",
      "CPU times: user 3.77 ms, sys: 2.29 ms, total: 6.06 ms\n",
      "Wall time: 490 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Provide the capital of Bangladesh.\",\n",
    "    max_tokens=10\n",
    ")\n",
    "answer = response['choices'][0]['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Bangladesh is Dhaka.\n",
      "CPU times: user 4.88 ms, sys: 3.19 ms, total: 8.07 ms\n",
      "Wall time: 637 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of Bangladesh?\"}],\n",
    "    max_tokens=10\n",
    ")\n",
    "answer = response['choices'][0]['message']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation of Models' Responses from the Uses Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Base Model was unsuccessful in all five tasks.\n",
    "- Results came fastest from the Base Model, followed by the Instruct and then the Chat Model.\n",
    "- Both the Instruct and Chat Models excelled in text generation.\n",
    "- While the Instruct Model provided its output in Bengali (as intended), the translation was inaccurate. In contrast, the Chat Model delivered a correct translation.\n",
    "- The Instruct Model outshined the Chat Model in summarization tasks.\n",
    "- In code generation, the Chat Model surpassed the Instruct Model by offering an interactive explanation and relevant examples. The Instruct Model did produce correct code with examples but lacked interactivity.\n",
    "- Both the Instruct and Chat Models accurately completed the question-answering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://platform.openai.com/docs/models\n",
    "- https://www.theinsaneapp.com/2023/05/instructgpt-vs-chatgpt.html\n",
    "- https://github.com/mneedham/LearnDataWithMark/blob/main/gpt-instruct/notebooks/GPT-Instruct.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
